{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c98606d",
   "metadata": {},
   "source": [
    "### Word2Vec + PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "745a4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from numpy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1eb9311e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Text.txt\") as file:\n",
    "    text_review = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12cb45f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The most common representation of distributional semantics is called one-hot representation in which dimensionality is equal to vocabularyâ€™s cardinality. Elements of this vector space representation consist of 0â€™s and 1â€™s. However, this representation has some disadvantages. For example, in these representations, it is difficult to make deductions about word similarity. Due to high dimensionality, they can also cause overfitting. Moreover, it is computationally expensive.\\nWord embeddings are designed to capture attributional similarities between vocabulary items. Words that appear in similar contexts should be close to each other in the projected vector space. This means that grouping of words in a vector space must share same semantic properties. In word embeddings, Latent Semantic Analysis (LSA) uses a counting base dimensionality reduction method. Word2Vec is created as an alternative. Its low dimensionality can help to reduce computational complexity. Also compared with distributional semantics methods, it causes less overfitting. Word2Vec can also detect analogies between words.\\nOur model takes Word2Vec representations of words in a vector space. While we construct the Word2Vec model, we decide a threshold of counts of words because words that appear only once or twice in a large corpus are probably not unusual for the model, and there is not enough data to make any meaningful training on those words. A reasonable value for minimum counts changes between 0-100, and it depends on the size of corpora. Another critical parameter for Word2Vec model is the dimension of the vectors. This value changes between 100 and 400. Dimensions larger than 400 require more training but leads to more accurate models. I used Google News corpora which provided by Google which consist of 3 million word vectors. I did not remove stop words or infrequent words because these algorithms use windows and to find vector representations. So I need the nearby words to find vector representations.\\nThe second step of this algorithm is to find PageRank value of each word. PageRank algorithm works with random walk. The original PageRank algorithm takes internet pages as a node. In our model PageRank algorithm takes Word2Vec representations of words. The cosine distance is used to calculate edge weights between nodes. After PageRank values of words are found, we can get words which have the highest PageRank values. Finally, these words can be seen as a keyword of a text.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23506f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want to use Google original vectors from Google News corpora\n",
    "#model = word2vec.Word2Vec.load_word2vec_format('GoogleNmews-vectors-negative300.bin', binary=True)\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
    "#if you want to use your own vector\n",
    "#model = word2vec.Word2Vec.load(\"w2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbfdc073",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text, remove_stopwords=True):\n",
    "    # 2. Remove non-letters\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "\n",
    "    # 3. Convert words to lower case and split them, clean stopwords from model' vocabulary\n",
    "    words = review_text.lower().split()\n",
    "    stops = set(stopwords.words('english'))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "\n",
    "# Function to get feature vec of words\n",
    "def get_feature_vec(words, model):\n",
    "    # Index2word is a list that contains the names of the words in\n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    #index2word_set = set(model.index2word)\n",
    "    index2word_set = set(model.index_to_key)\n",
    "    clean_text = []\n",
    "    # vocabulary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            clean_text.append(model[word])\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c5fd86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of word list without stopwords\n",
    "clean_train_text = (text_to_wordlist(text_review, remove_stopwords=True))\n",
    "\n",
    "# delete words which occur more than ones\n",
    "clean_train = []\n",
    "for words in clean_train_text:\n",
    "    if words in clean_train:\n",
    "        words = +1\n",
    "    else:\n",
    "        clean_train.append(words)\n",
    "\n",
    "trainDataVecs = get_feature_vec(clean_train, model)\n",
    "trainData = numpy.asarray(trainDataVecs)\n",
    "\n",
    "# calculate cosine similarity matrix to use in pagerank algorithm for dense matrix, it is not\n",
    "# fast for sparse matrix\n",
    "# sim_matrix = 1-pairwise_distances(trainData, metric=\"cosine\")\n",
    "\n",
    "# similarity matrix, it is 30 times faster for sparse matrix\n",
    "# replace this with A.dot(A.T).todense() for sparse representation\n",
    "similarity = numpy.dot(trainData, trainData.T)\n",
    "\n",
    "# squared magnitude of preference vectors (number of occurrences)\n",
    "square_mag = numpy.diag(similarity)\n",
    "\n",
    "# inverse squared magnitude\n",
    "inv_square_mag = 1 / square_mag\n",
    "\n",
    "# if it doesn't occur, set it's inverse magnitude to zero (instead of inf)\n",
    "inv_square_mag[numpy.isinf(inv_square_mag)] = 0\n",
    "\n",
    "# inverse of the magnitude\n",
    "inv_mag = numpy.sqrt(inv_square_mag)\n",
    "\n",
    "# cosine similarity (elementwise multiply by inverse magnitudes)\n",
    "cosine = similarity * inv_mag\n",
    "cosine = cosine.T * inv_mag\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c48750bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pagerank powermethod\n",
    "def powerMethod(A, x0, m, iter):\n",
    "    n = A.shape[1]\n",
    "    delta = m * (array([1] * n, dtype='float64') / n)\n",
    "    for i in range(iter):\n",
    "        x0 = dot((1 - m), dot(A, x0)) + delta\n",
    "    return x0\n",
    "\n",
    "\n",
    "n = cosine.shape[1]  # A is n x n\n",
    "m = 0.15\n",
    "x0 = [1] * n\n",
    "\n",
    "pagerank_values = powerMethod(cosine, x0, m, 130)\n",
    "\n",
    "srt = numpy.argsort(-pagerank_values)\n",
    "a = srt[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5249c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vocabulary', 'vocabulary', 'vec', 'decide', 'value', 'corpus', 'words', 'high', 'analogies', 'threshold']\n"
     ]
    }
   ],
   "source": [
    "keywords_list = []\n",
    "\n",
    "for words in a:\n",
    "    keywords_list.append(clean_train_text[words])\n",
    "    \n",
    "print (keywords_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ff6f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744c54a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da54169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
